---
title: "vignette"
author: 
- name: Jiefei Wang
  affiliation: Roswell Park Comprehensive Cancer Center, Buffalo, NY
date: "`r Sys.Date()`"
output:
    BiocStyle::html_document:
        toc: true
        toc_float: true
vignette: >
  %\VignetteIndexEntry{vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
package: DockerParallel
---

```{r, include = FALSE}
if(!"root_dir"%in%ls()){
  root_dir<-FALSE
}
# root_dir<-TRUE;knitr::knit("vignettes/vignette.Rmd", output = "README.md")
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(DockerParallel)
if(root_dir){
  component_img <- "vignettes/components.jpg"
  fargate_img <- "vignettes/fargate.png"
  traditional_img <- "vignettes/traditional.png"
}else{
  component_img <- "components.jpg"
  fargate_img <- "fargate.png"
  traditional_img <- "traditional.png"
}
DockerParallel:::set_retry_time(10)
DockerParallel:::set_print_when_retrying(FALSE)
# knitr::knit("vignettes/vignette.Rmd", output = "README.md")
```

# Introduction
Parallel computing has became an important tool to analysis large and complex data. Using the `parallel` package to create local computing cluster is probably the most well-known method for the parallel computing in R's realm. As the advance of the cloud computing, there is a natural need to make R parallel compatible with the cloud. `DockerParallel` is a package that is designed for the cloud computation. It aims to provide easy-to-learn, highly scalable and low-cost tools to make the cloud computation possible.

The core component of `DockerParallel`, as its name implies, is the docker container. Container is a technique to package up code and all its dependencies in a standard unit and run it in an isolated environment from the host OS. By containerizing R's worker node, `DockerParallel` can easily deploy hundreds of identical workers in a cloud environment regardless of the host hardware and OS that run the nodes. In this vignette, we will demonstrate how to use `DockerParallel` to run a cluster using Amazon Elastic Compute Service(ECS).

# The structure of `DockerParallel`
For understanding the structure of `DockerParallel`, imagine that if someone tells you to create a containerized R parallel cluster on the cloud, what question you will ask before you can deploy the cluster on the cloud? Generally speaking, there are three questions you should have in mind

1. Which container should be used?
2. Who provides the container service?
3. What is the cluster configuration(e.g. worker number, CPU, memory)?

`DockerParallel` answers these three questions via three components: `CloudProvider`, `Container` and `CloudConfig`. These components can be summarized in the following figure

![](`r component_img`)

We would not discuss the details of these component in this vignette, if users are interested in developing new cloud provider or container, please read `cookbook for developers` in the package.

# The structure of ECS
Amazon provides Elastic Compute Service to take over the management of servers. By using ECS, the user only needs to prepare the container image and ECS will find the best server to run the container. ECS provides both the traditional server and fargate as the host machine of the container. For the traditional server, the user is able to select a set of hardware that can run the container. For the fargate launch type, it does not refer to any particular server. The choice of the server is determined by Amazon and is opaque to the user. The user only need to specify the CPU and memory that a container needs. Therefore, it greatly simplifies the deployment of the container. 

In this vignette, We use the foreach redis parallel backend and deploy the container using the ECS fargate launch type. Below is the diagram of how `DockerParallel` works with ECS and Bioconductor foreach redis container

![](`r fargate_img`)

The cluster object is created in your local R session, but the workers and redis server are from Amazon ECS. Each docker container contains one or more R workers, they will receive jobs sent by your local R session and do the parallel computing. The workflow of the `DockerParallel` package is as follow

1. Select a cloud provider(default: ECSProvider)
2. Select a container(default: BiocFERContainer)
3. Create the cluster and run your parallel task

In the rest of the vignette we will introduce them step by step


# Set the ssh key pair


The first step of using the package is to set the ssh key. The public key should be known by the container so it will allow the connection from your local cluster. You can set the path to the key file via `set_ssh_key(private_key = "private key path", public_key = "public key path")`. If no argument is provided to `set_ssh_key`, it will find the key by the environment variables `DockerParallelSSHPrivateKey` and `DockerParallelSSHPublicKey`
```
set_ssh_key()
```
Note that if you use the environment variable to set the ssh key, it will be automatically located by the package, there is no need to call `set_ssh_key`.


# Authentication

For communicating with the cloud, you need to authenticate with the cloud provider. Amazon cloud uses `access key id` and `secret access key` to verify your identity. You can find the instruction on how to download your credentials from [AWS Documentation]. Once you have the credentials, you can specify them by
```{r}
ecs_set_credentials()
```
`ecs_set_credentials` will determine your credentials as well as the region of the cloud service. The region is the physical location of the cloud servers that will run your worker nodes. The function uses a variety of ways to find such information. The most important methods are as follow(sorted by the search order):

1. user-supplied values passed to the function

2. environment variables `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_DEFAULT_REGION`, and `AWS_SESSION_TOKEN`

You can either explicitly specify them or provide them as environment variables.

[AWS Documentation]: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey

# configure worker node
The next step is to configure the hardware and docker image of your worker nodes. The information of the worker nodes is stored as an S4 class, you can get a default one from
```{r}
config <- ecs_configuration()
config
```
The most important settings are the CPU and memory of the worker node. The value for the CPU represents the CPU units(CU) used by a worker node. A single CPU core corresponds to 1024 CU. The unit for the memory is MB. The default setting gives you the minimum hardware for a worker node. You can specify up to 4096 CU and 30GB memory for a worker. Note that not all combinations of the CPU and memory are valid in ECS. If you provide an invalid combination, the resulting workers will have at least that much CPU units and memory you specified. Please see [AWS Fargate limit] For the valid combinations. The image is the image containing the worker node. Currently this is a fixed value, but in future we will allow users to choose a customized image.

The cost of the workers will be based on the duration of the workers. Pricing is per second with a 1-minute minimum. The unit price for the workers will be calculated from the region, CPU and memory that you specified. You can find more detail from [AWS Fargate Pricing]

[AWS Fargate Pricing]: https://aws.amazon.com/fargate/pricing/
[AWS Fargate limit]: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html

# Create cluster
The last step is to create your cluster. It can be done by
```{r}
cl <- docker_cluster(config, workers = 2)
cl
```
The function takes the `config` object as the worker template to create workers on ECS. The variable `workers` determine the number of workers in the cluster. It will create a cluster with 2 worker nodes. After the cluster is created, you can treat the cluster as the one returned by `parallel::makeCluster` and do your computation as usual. For example
```{r}
parallel::parLapply(cl, 1:4, function(i) runif(i))
```
This will dispatch your parallel task to the worker nodes you just created. After you finish your work, the cluster can be closed by
```{r}
parallel::stopCluster(cl)
```

# Session info
```{r}
sessionInfo()
```

